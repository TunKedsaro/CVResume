{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b085a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Dict, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da8477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriterionScore(BaseModel):\n",
    "    score: int = Field(ge=0, le=5)\n",
    "    feedback: str\n",
    "class SectionEvaluation(BaseModel):\n",
    "    section: Literal[\"Skills\"]\n",
    "    scores: Dict[str, CriterionScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_llm_output(raw_output: dict) -> SectionEvaluation:\n",
    "    return SectionEvaluation.model_validate(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1aa90d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "op1 = {\n",
    "    \"section\":\"Skills\",\n",
    "    \"scores\": {\n",
    "        \"Completeness\"  : { \"score\": 0, \"feedback\": \"abc\" },\n",
    "        \"Length\"        : { \"score\": 1, \"feedback\": \"def\" },\n",
    "        \"RoleRelevance\" : { \"score\": 2, \"feedback\": \"ghi\" }\n",
    "    }\n",
    "}\n",
    "\n",
    "op2 = {\"abc\":\"xyx\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e6b14a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SectionEvaluation(section='Skills', scores={'Completeness': CriterionScore(score=0, feedback=''), 'Length': CriterionScore(score=0, feedback=''), 'RoleRelevance': CriterionScore(score=0, feedback='')})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_llm_output(op1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef02468",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for SectionEvaluation\nsection\n  Field required [type=missing, input_value={'abc': 'xyx'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nscores\n  Field required [type=missing, input_value={'abc': 'xyx'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m validate_llm_output(op2)\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mvalidate_llm_output\u001b[1;34m(raw_output)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_llm_output\u001b[39m(raw_output: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SectionEvaluation:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SectionEvaluation\u001b[38;5;241m.\u001b[39mmodel_validate(raw_output)\n",
      "File \u001b[1;32mc:\\Users\\TunKedsaro\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:716\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, extra, from_attributes, context, by_alias, by_name)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    713\u001b[0m         code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate-by-alias-and-name-false\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    714\u001b[0m     )\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(\n\u001b[0;32m    717\u001b[0m     obj,\n\u001b[0;32m    718\u001b[0m     strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[0;32m    719\u001b[0m     extra\u001b[38;5;241m=\u001b[39mextra,\n\u001b[0;32m    720\u001b[0m     from_attributes\u001b[38;5;241m=\u001b[39mfrom_attributes,\n\u001b[0;32m    721\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    722\u001b[0m     by_alias\u001b[38;5;241m=\u001b[39mby_alias,\n\u001b[0;32m    723\u001b[0m     by_name\u001b[38;5;241m=\u001b[39mby_name,\n\u001b[0;32m    724\u001b[0m )\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for SectionEvaluation\nsection\n  Field required [type=missing, input_value={'abc': 'xyx'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nscores\n  Field required [type=missing, input_value={'abc': 'xyx'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing"
     ]
    }
   ],
   "source": [
    "validate_llm_output(op2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d77cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ip = op1\n",
    "try:\n",
    "    validate_llm_output(ip)\n",
    "    print(\"1\")\n",
    "except:\n",
    "    validate_llm_output(ip)\n",
    "    print(\"2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11595714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "371bcd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d69aa419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip -> {'abc': 'xyx'}\n",
      "0 → retry\n",
      "Output error recall again ...\n",
      "====================================================================================================\n",
      "ip -> {'abc': 'xyx'}\n",
      "0 → retry\n",
      "Output error recall again ...\n",
      "====================================================================================================\n",
      "ip -> {'abc': 'xyx'}\n",
      "0 → retry\n",
      "Output error recall again ...\n",
      "====================================================================================================\n",
      "ip -> {'abc': 'xyx'}\n",
      "0 → retry\n",
      "Output error recall again ...\n",
      "====================================================================================================\n",
      "ip -> {'section': 'Skills', 'scores': {'Completeness': {'score': 0, 'feedback': 'abc'}, 'Length': {'score': 1, 'feedback': 'def'}, 'RoleRelevance': {'score': 2, 'feedback': 'ghi'}}}\n",
      "1\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    ip = [op1, op2][randint(0,1)]\n",
    "    print(\"ip ->\", ip)\n",
    "\n",
    "    try:\n",
    "        validate_llm_output(ip)\n",
    "        print(\"1\")\n",
    "        break\n",
    "\n",
    "    except ValidationError:\n",
    "        print(\"0 → retry\")\n",
    "        print(\"Output error recall again ...\")\n",
    "        continue\n",
    "\n",
    "    finally:\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59061c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e089f3ab",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "{\n",
    "    \"response\": {\n",
    "        \"section\": \"Profile\",\n",
    "        \"total_score\": 16,\n",
    "        \"scores\": {\n",
    "        \"ContentQuality\": {\n",
    "            \"score\": 6,\n",
    "            \"feedback\": \"The summary clearly outlines data science focus and business impact but lacks quantifiable results and specific methodologies within the introductory statements.\"\n",
    "        },\n",
    "        \"Completeness\": {\n",
    "            \"score\": 10,\n",
    "            \"feedback\": \"The profile successfully establishes the candidate's professional identity as a data scientist with a clear career direction and relevant focus areas.\"\n",
    "        }\n",
    "        }\n",
    "    },\n",
    "    \"response_time\": \"5.95040 s\",\n",
    "    \"estimated_cost_thd\": \"0.02793 ฿\"\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bfbd18f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
