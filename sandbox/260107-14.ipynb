{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ae0466",
   "metadata": {},
   "source": [
    "260107.16.49\n",
    "This file create for try to find solution for global feeback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6539bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24c1ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import os\n",
    "\n",
    "class LiteralString(str):\n",
    "    \"\"\"Force YAML to use block literal style '|'.\"\"\"\n",
    "    pass\n",
    "def literal_representer(dumper,value):\n",
    "    \"\"\"Represent LiteralString using YAML block style.\"\"\"\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', value, style='|')\n",
    "\n",
    "# Register custom representer\n",
    "yaml.add_representer(LiteralString, literal_representer)\n",
    "\n",
    "class Helper:\n",
    "    \"\"\"\n",
    "    Utility helper class for file I/O, YAML/JSON handling,\n",
    "    formatting, and configuration transformations.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def load_file(filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Load and return the contents of a text file.\n",
    "        Args:\n",
    "            filepath (str): Path to the file.\n",
    "        Returns:\n",
    "            str: File contents.\n",
    "        \"\"\"\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_yaml(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Load a YAML file and return its contents as a dictionary.\n",
    "        Args:\n",
    "            filepath (str): Path to the YAML file.\n",
    "        Returns:\n",
    "            dict: Parsed YAML content.\n",
    "        \"\"\"\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return yaml.safe_load(f)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_json(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Load a JSON file and return its contents as a dictionary.\n",
    "        Args:\n",
    "            filepath (str): Path to the JSON file.\n",
    "        Returns:\n",
    "            dict: Parsed JSON content.\n",
    "        \"\"\"\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    @staticmethod\n",
    "    def save_yaml(newconfig,filepath: str):\n",
    "        \"\"\"\n",
    "        Save a dictionary to a YAML file.\n",
    "        Preserves key order and applies custom representers\n",
    "        such as block literal formatting.\n",
    "        Args:\n",
    "            newconfig (dict): Configuration data to save.\n",
    "            filepath (str): Output YAML file path.\n",
    "        \"\"\"\n",
    "        with open(filepath,\"w\",encoding=\"utf-8\") as file:\n",
    "            return yaml.dump(newconfig,file,sort_keys=False)\n",
    "        \n",
    "    @staticmethod\n",
    "    def fop(num: float) -> float:\n",
    "        \"\"\"\n",
    "        Format a number to one decimal place.\n",
    "        Args:\n",
    "            num (float): Input number.\n",
    "        Returns:\n",
    "            float: Rounded number with one decimal precision.\n",
    "        \"\"\"\n",
    "        return float(f\"{num:.1f}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def prettyjson(txt:str) -> str:\n",
    "        \"\"\"\n",
    "        Pretty-print a JSON-compatible object as a formatted string.\n",
    "        Args:\n",
    "            txt (str): JSON-compatible object.\n",
    "        Returns:\n",
    "            str: Indented JSON string.\n",
    "        \"\"\"\n",
    "        return str(json.dumps(txt,indent=4, ensure_ascii=False))\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_literal(value):\n",
    "        \"\"\"\n",
    "        Convert multiline strings into LiteralString for YAML output.\n",
    "        Single-line strings and non-string values are returned unchanged.\n",
    "        \"\"\"\n",
    "        if isinstance(value,str) and \"\\n\" in value:\n",
    "            return LiteralString(value)\n",
    "        return value\n",
    "    \n",
    "    @staticmethod\n",
    "    def deep_literal_transform(data):\n",
    "        '''\n",
    "        Recursively convert ALL multiline strings inside dict/list\n",
    "        into LiteralString so block formatting is preserved\n",
    "        '''\n",
    "        if isinstance(data, dict):\n",
    "            return {k: Helper.deep_literal_transform(v) for k,v in data.items()}\n",
    "        if isinstance(data, list):\n",
    "            return [Helper.deep_literal_transform(i) for i in data]\n",
    "        # Convert only multiline string\n",
    "        return Helper.to_literal(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "317ee21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "class PromptBuilder(Helper):\n",
    "    \"\"\"\n",
    "    Builds a structured evaluation prompt for a specific resume section.\n",
    "    Uses YAML-driven configuration to assemble role instructions,\n",
    "    evaluation criteria, scoring scale, expected content, and\n",
    "    the candidate resume into a single LLM-ready prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, section, criteria, targetrole, cvresume, include_fewshot: bool = True, output_lang = \"en\"):\n",
    "        \"\"\"\n",
    "        Initialize the prompt builder for a resume section.\n",
    "        Args:\n",
    "            section (str): Resume section to evaluate (e.g. \"Education\").\n",
    "            criteria (list): Evaluation criteria for the section.\n",
    "            targetrole (str): Target role used for role relevance.\n",
    "            cvresume (str): Resume content to be evaluated.\n",
    "            include_fewshot (bool): Whether to include example scores.\n",
    "            output_lang (str): Output language code (e.g. \"en\", \"th\").\n",
    "        \"\"\"\n",
    "        self.section        = section\n",
    "        self.criteria       = criteria[::-1]\n",
    "        self.cvresume       = cvresume\n",
    "        self.targetrole     = targetrole\n",
    "        self.include_fewshot= include_fewshot\n",
    "        self.output_lang    = output_lang\n",
    "        \n",
    "        self.config = self.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\prompt.yaml\")\n",
    "        self.criteria_cfg = self.config.get(\"criteria\", {})\n",
    "\n",
    "    def build_response_template(self):\n",
    "        \"\"\"\n",
    "        Build an empty JSON response template for the LLM.\n",
    "        Returns:\n",
    "            dict: Response skeleton with section name and criteria scores.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"section\": self.section,\n",
    "            \"scores\": {\n",
    "                c: {\"score\": 0, \"feedback\": \"\"} for c in self.criteria\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _build_criteria_block(self) -> str:\n",
    "        \"\"\"\n",
    "        Construct the criteria description block for the prompt.\n",
    "        Includes few-shot score examples if enabled and available\n",
    "        in the prompt configuration.\n",
    "        Returns:\n",
    "            str: Formatted criteria block text.\n",
    "        \"\"\"\n",
    "        blocks = []\n",
    "        for crit in self.criteria:\n",
    "            block = f\"- {crit}\\n\"\n",
    "            if self.include_fewshot and crit in self.criteria_cfg:\n",
    "                few_cfg = self.criteria_cfg[crit]\n",
    "                for score in [5, 3, 1]:\n",
    "                    key = f\"score{score}\"\n",
    "                    if key in few_cfg:\n",
    "                        text = few_cfg[key].strip()\n",
    "                        block += f\"    score {score}: {text}\\n\"\n",
    "            blocks.append(block)\n",
    "        return \"\".join(blocks)\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Assemble the full evaluation prompt.\n",
    "        Combines role instructions, evaluation objectives, section context,\n",
    "        criteria definitions, scoring scale, expected output format,\n",
    "        and resume content into a single prompt string.\n",
    "        Returns:\n",
    "            str: Final prompt ready to be sent to the LLM.\n",
    "        \"\"\"\n",
    "        config_role      = self.config['role']['role1']\n",
    "        config_objective = self.config['objective']['objective1']\n",
    "        config_section   = self.config['section']['section1']\n",
    "        config_expected  = self.config['expected_content'][self.section]\n",
    "        config_scale     = self.config['scale']['score1']\n",
    "        criteria_block   = self._build_criteria_block()\n",
    "        config_lang      = self.config['Language_output_style'][self.output_lang]\n",
    "\n",
    "        prompt_role      = f\"Role :\\n{config_role}\\n\\n\"\n",
    "        prompt_objective = f\"Objectvie :\\n{config_objective}\\n\"\n",
    "        promnt_lang      = f\"Output Language Instruction::\\n{config_lang}\\n\"\n",
    "        prompt_section   = f\"Section :\\n{config_section}\\n\\n\"\n",
    "        prompt_expected  = f\"Expected :\\n{config_expected}\\n\"\n",
    "        prompt_criteria  = f\"Criteria :\\n{criteria_block}\\n\"\n",
    "        prompt_scale     = f\"Scale :\\n{config_scale}\\n\"\n",
    "        prompt_output    = f\"Otput :\\n{json.dumps(self.build_response_template(), indent=2)}\\n\\n\"\n",
    "        prompt_cvresume  = f\"CV/Resume: \\n{self.cvresume}\\n\"\n",
    "        prompt = (\n",
    "            prompt_role + prompt_objective + prompt_section + promnt_lang\n",
    "            + prompt_expected + prompt_criteria + prompt_scale\n",
    "            + prompt_output + prompt_cvresume \n",
    "        )\n",
    "        # print(f\"prompt -> \\n{prompt}\")\n",
    "        prompt = prompt.replace(\"<section_name>\", self.section)\n",
    "        prompt = prompt.replace(\"<targetrole>\", self.targetrole)\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1e562bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_resume(payload):\n",
    "    resume_json = payload.resume_json\n",
    "    targetrole  = payload.target_role\n",
    "    output_lang = payload.output_lang \n",
    "\n",
    "    p1 = PromptBuilder(\n",
    "        section     = \"Profile\",\n",
    "        criteria    = [\"Completeness\", \"ContentQuality\"],\n",
    "        targetrole  = targetrole,\n",
    "        cvresume    = resume_json,\n",
    "        output_lang = output_lang \n",
    "    )\n",
    "    prompt1 = p1.build()\n",
    "    \n",
    "    p2 = PromptBuilder( \n",
    "        section     = \"Summary\", \n",
    "        criteria    = [\"Completeness\", \"ContentQuality\",\"Grammar\",\"Length\",\"RoleRelevance\"],\n",
    "        targetrole  = targetrole,\n",
    "        cvresume    = resume_json,\n",
    "        output_lang = output_lang \n",
    "    )\n",
    "    prompt2 = p2.build()\n",
    "\n",
    "    # op1,raw1 = caller.call(prompt1)\n",
    "    # op2,raw2 = caller.call(prompt2)\n",
    "\n",
    "    # s1 = agg.aggregate(op1)\n",
    "    # s2 = agg.aggregate(op2)\n",
    "\n",
    "    # x = GlobalAggregator(SectionScoreAggregator_output = [s1,s2])\n",
    "\n",
    "    # output = x.fn0()\n",
    "\n",
    "\n",
    "    # return {\n",
    "    #     \"response\": output,\n",
    "    #     \"response_time\" : f\"{usage_time:.5f} s\",\n",
    "    #     \"estimated_cost_thd\": f\"{(cost1['total_cost']+cost2['total_cost']+cost3['total_cost']+cost4['total_cost']+cost5['total_cost']+cost6['total_cost'])*usd2bath:.5f} à¸¿\"\n",
    "    # }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Dict\n",
    "\n",
    "class CriterionScore(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a single evaluation score for one criterion.\n",
    "    Attributes:\n",
    "        score (int): Integer score between 0 and 5.\n",
    "        feedback (str): Textual feedback explaining the score.\n",
    "    \"\"\"\n",
    "    score: int = Field(ge=0, le=5)\n",
    "    feedback: str\n",
    "\n",
    "class SectionEvaluation(BaseModel):\n",
    "    \"\"\"\n",
    "    Validated LLM evaluation output for a resume section.\n",
    "    Attributes:\n",
    "        section (str): Name of the resume section.\n",
    "        scores (Dict[str, CriterionScore]): Mapping of criteria to scores.\n",
    "    \"\"\"\n",
    "    section: str\n",
    "    scores: Dict[str, CriterionScore]\n",
    "\n",
    "class LlmCaller(Helper):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Handles interaction with the LLM for section-level resume evaluation.\n",
    "        Responsibilities:\n",
    "        - Send prompts to the LLM\n",
    "        - Parse and validate structured JSON responses\n",
    "        - Retry with a repair prompt if validation fails\n",
    "        - Return validated output along with raw LLM response\n",
    "        \"\"\"\n",
    "        self.model_cfg = self.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\model.yaml\")\n",
    "        self.client    = genai.Client(api_key='AIzaSyARN...L2b-3dHAWb5urRk')\n",
    "        self.model     = self.model_cfg[\"model\"][\"generation_model\"]\n",
    "        self.usd2bath  = Helper.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\global.yaml\")['currency']['USD_to_THB']\n",
    "        self.log_digit = Helper.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\global.yaml\")['logging']['logging_round_digit']\n",
    "\n",
    "    def _parse(self, resp):\n",
    "        \"\"\"\n",
    "        Parse raw LLM response text into a JSON object.\n",
    "        Removes markdown code fences and converts the result to a dictionary.\n",
    "        Args:\n",
    "            resp: Raw response object from the LLM client.\n",
    "        Returns:\n",
    "            dict: Parsed JSON output.\n",
    "        \"\"\"\n",
    "        text = resp.text.strip()\n",
    "        text = re.sub(r\"^```json|```$\", \"\", text).strip()\n",
    "        return json.loads(text)\n",
    "    \n",
    "    def _call_raw(self, prompt: str):\n",
    "        \"\"\"\n",
    "        Send a prompt to the LLM without validation.\n",
    "        Args:\n",
    "            prompt (str): Prompt text to send.\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - Parsed JSON output (dict)\n",
    "                - Raw LLM response object\n",
    "        \"\"\"\n",
    "        resp = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=prompt\n",
    "        )\n",
    "        parsed = self._parse(resp)\n",
    "        return parsed, resp\n",
    "    \n",
    "    def _validate(self, raw_output:dict)->SectionEvaluation:\n",
    "        \"\"\"\n",
    "        Validate LLM output against the expected schema.\n",
    "        Args:\n",
    "            raw_output (dict): Parsed LLM output.\n",
    "        Returns:\n",
    "            SectionEvaluation: Validated evaluation object.\n",
    "        Raises:\n",
    "            ValidationError: If the output does not match the schema.\n",
    "        \"\"\"\n",
    "        return SectionEvaluation.model_validate(raw_output)\n",
    "    \n",
    "    def _repair_prompt(self, error_msg: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a corrective prompt when LLM output validation fails.\n",
    "        Instructs the LLM to strictly follow the expected JSON schema.\n",
    "        Args:\n",
    "            error_msg (str): Validation error message.\n",
    "        Returns:\n",
    "            str: Repair prompt to prepend to the original prompt.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "                Your previous response was INVALID.\n",
    "                Validation error:\n",
    "                {error_msg}\n",
    "                STRICT RULES:\n",
    "                - Return JSON only\n",
    "                - No markdown\n",
    "                - No explanation\n",
    "                - Follow schema exactly\n",
    "                - Section name must start with a capital letters (e.g. \"Education\")\n",
    "                Expected format:\n",
    "                {{\n",
    "                    \"section\": \"<section_name>\",\n",
    "                    \"scores\": {{\n",
    "                        \"<criterion>\": {{ \"score\": 0-5, \"feedback\": \"string\" }}\n",
    "                    }}\n",
    "                }}\n",
    "        \"\"\"\n",
    "    \n",
    "    def call(self,prompt:str, max_retry:int = 3):\n",
    "        \"\"\"\n",
    "        Call the LLM with validation and automatic retry.\n",
    "        Attempts to validate the LLM response. If validation fails,\n",
    "        retries using a repair prompt up to the specified limit.\n",
    "        Args:\n",
    "            prompt (str): Prompt text to send.\n",
    "            max_retry (int): Maximum number of retry attempts.\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - Validated evaluation result (dict)\n",
    "                - Raw LLM response object\n",
    "        \"\"\"\n",
    "        last_error = None\n",
    "        repair_prompt = \"\\n\"\n",
    "        for attemp in range(max_retry):\n",
    "            final_prompt = repair_prompt + prompt\n",
    "            # print(f\"final_prompt attemp : {attemp} -> \\n {final_prompt}\")\n",
    "            output, raw = self._call_raw(final_prompt)\n",
    "            try:\n",
    "                validated = self._validate(output)\n",
    "                # print('Status : 1')\n",
    "                return validated.model_dump(),raw\n",
    "            except ValidationError as e:\n",
    "                last_error = str(e)\n",
    "                repair_prompt = self._repair_prompt(last_error)\n",
    "                # print('Status : 0')\n",
    "                print(\"Output error recall again ...\")\n",
    "            finally:\n",
    "                print('='*100)\n",
    "        return {\n",
    "            \"section\":\"UNKNOW\",\n",
    "            \"scores\":{}\n",
    "        },raw\n",
    "\n",
    "    async def call_async(self, prompt: str):\n",
    "        \"\"\"\n",
    "        Asynchronous wrapper for `call`.\n",
    "        Executes the blocking LLM call in a background thread.\n",
    "        Args:\n",
    "            prompt (str): Prompt text to send.\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - Validated evaluation result (dict)\n",
    "                - Raw LLM response object\n",
    "        \"\"\"\n",
    "        return await asyncio.to_thread(self.call, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timezone,timedelta\n",
    "import copy\n",
    "\n",
    "class SectionScoreAggregator(Helper):\n",
    "    \"\"\"\n",
    "    Aggregates raw LLM criterion scores for a single resume section\n",
    "    into weighted scores and a section-level total score (0-5).\n",
    "    Configuration:\n",
    "        - src/config/weight.yaml\n",
    "    Expected LLM Output Format (input):\n",
    "        {\n",
    "            \"section\": \"<section_name>\",\n",
    "            \"scores\": {\n",
    "                \"<criterion>\": {\n",
    "                    \"score\": int,        # 0-5\n",
    "                    \"feedback\": str\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    Output Format:\n",
    "        {\n",
    "            \"section\": \"<section_name>\",\n",
    "            \"total_score\": float,\n",
    "            \"scores\": {\n",
    "                \"<criterion>\": {\n",
    "                    \"score\": float,      # weighted score\n",
    "                    \"feedback\": str\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.config          = self.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\weight.yaml\")         # config\n",
    "    def aggregate(self,llm_output:dict):\n",
    "        self.llm_output      = llm_output             # op\n",
    "        self.section         = llm_output[\"section\"]  # Get section\n",
    "        self.section_weights = self.config[\"weights\"][self.section]  # config[\"weights\"][section_key][criteria]\n",
    "        ddict = {}\n",
    "        total = 0.0\n",
    "        # Protect multiple mutation when we run more than one time\n",
    "        scores_copy = copy.deepcopy(self.llm_output[\"scores\"])\n",
    "        for criteria, body in scores_copy.items():\n",
    "            raw = body[\"score\"]\n",
    "            w   = self.section_weights[criteria]\n",
    "            weighted = raw / 5 * w\n",
    "            body[\"score\"] = weighted\n",
    "            ddict[criteria] = body\n",
    "            total = total + weighted\n",
    "        return {\n",
    "            \"section\": self.section,\n",
    "            \"total_score\":total,\n",
    "            \"scores\":ddict\n",
    "        }\n",
    "    \n",
    "    \n",
    "class GlobalAggregator(Helper):\n",
    "    \"\"\"\n",
    "    Aggregates section-level evaluation results into a final resume score.\n",
    "    Combines weighted section scores, detailed per-section breakdowns,\n",
    "    and evaluation metadata (model, configuration versions, timestamp).\n",
    "    Inputs:\n",
    "        SectionScoreAggregator_output (list):\n",
    "            Section-level aggregation results.\n",
    "    Configurations:\n",
    "        - model.yaml   : LLM model information\n",
    "        - weight.yaml  : Section weights and version\n",
    "        - prompt.yaml  : Prompt configuration version\n",
    "    Input : \n",
    "    s1 = {\n",
    "    'section': 'Experience',\n",
    "    'total_score': 78.0,\n",
    "    'scores': {\n",
    "        'RoleRelevance':  {'score': 30.0, 'feedback': 'xxx'},\n",
    "        'Length':         {'score': 8.0, 'feedback': 'yyy'},\n",
    "        'Grammar':        {'score': 10.0, 'feedback': 'zzz'},\n",
    "        'ContentQuality': {'score': 24.0, 'feedback': 'aaa'},\n",
    "        'Completeness':   {'score': 6.0, 'feedback': 'bbb'}\n",
    "            }\n",
    "        }\n",
    "    s2 = {\n",
    "        'section': 'Profile',\n",
    "        'total_score': 90.0,\n",
    "        'scores': {\n",
    "            'RoleRelevance':  {'score': 20.0, 'feedback': 'xxx'},\n",
    "            'Length':         {'score': 10.0, 'feedback': 'yyy'},\n",
    "            'Grammar':        {'score': 25.0, 'feedback': 'zzz'},\n",
    "            'ContentQuality': {'score': 22.0, 'feedback': 'aaa'},\n",
    "            'Completeness':   {'score': 13.0, 'feedback': 'bbb'}\n",
    "                }\n",
    "            }\n",
    "\n",
    "    Output :\n",
    "    Ss = {\n",
    "        \"conclution\":{\n",
    "            \"final_resume_score\": 82.4,\n",
    "            \"section_contribution\":{\n",
    "                \"Profile\":{\n",
    "                    \"section_total\":78.0,\n",
    "                    \"section_weight\":40,\n",
    "                    \"contribution\":31.2\n",
    "                },\n",
    "                \"Experience\":{\n",
    "                    \"section_total\":90.0,\n",
    "                    \"section_weight\":10,\n",
    "                    \"contribution\":9.0\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"section_details\":{\n",
    "            \"Profile\":{\n",
    "                \"total_score\":78.0,\n",
    "                \"scores\":{\n",
    "                        'RoleRelevance':  {'score': 30.0, 'feedback': 'xxx'},\n",
    "                        'Length':         {'score': 8.0, 'feedback': 'yyy'},\n",
    "                        'Grammar':        {'score': 10.0, 'feedback': 'zzz'},\n",
    "                        'ContentQuality': {'score': 24.0, 'feedback': 'aaa'},\n",
    "                        'Completeness':   {'score': 6.0, 'feedback': 'bbb'}\n",
    "                }\n",
    "            },\n",
    "            \"Experience\":{\n",
    "                \"total_score\":90.0,\n",
    "                \"scores\":{\n",
    "                        'RoleRelevance':  {'score': 20.0, 'feedback': 'xxx'},\n",
    "                        'Length':         {'score': 10.0, 'feedback': 'yyy'},\n",
    "                        'Grammar':        {'score': 25.0, 'feedback': 'zzz'},\n",
    "                        'ContentQuality': {'score': 22.0, 'feedback': 'aaa'},\n",
    "                        'Completeness':   {'score': 13.0, 'feedback': 'bbb'}\n",
    "                }      \n",
    "            }\n",
    "        },\n",
    "        \"metadata\":{\n",
    "            \"model_name\":\"gemini-2.5-flash\",\n",
    "            \"timestamp\": \"2025-12-03T11:45:00+07:00\",\n",
    "            # \"processing_time_ms\":1234,\n",
    "            # \"input_tokens\":1234,\n",
    "            # \"output_tokens\":435,\n",
    "            # \"total_cost_usd\":0.001234,\n",
    "            \"weights_version\":\"weights_v1\",\n",
    "            \"prompt_version\":\"prompt_v1\"\n",
    "        }\n",
    "    }                                                                                       \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,SectionScoreAggregator_output:list):\n",
    "        self.section_outputs = SectionScoreAggregator_output\n",
    "        self.timestamp       = str(datetime.now(tz=(timezone(timedelta(hours=7)))))\n",
    "        self.model_config    = Helper.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\model.yaml\")     # should include model name\n",
    "        self.weight_config   = Helper.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\weight.yaml\")    # includes weights + version\n",
    "        self.prompt_config   = Helper.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\prompt.yaml\")  \n",
    "        self.model_cfg       = self.load_yaml(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\config\\model.yaml\")\n",
    "        self.client    = genai.Client(api_key='AIzaSyARNu...gL2b-3dHAWb5urRk')\n",
    "        self.model     = self.model_cfg[\"model\"][\"generation_model\"]\n",
    "    # includes prompt version\n",
    "    def fn1(self,globalfeedback):\n",
    "        \"\"\"\n",
    "        Calculate the final resume score using section-level weights.\n",
    "        Applies section weights to each section's total score and\n",
    "        computes the overall composite resume score.\n",
    "        Returns:\n",
    "            dict:\n",
    "                Final resume score and per-section contribution details.\n",
    "        \"\"\"\n",
    "        weights = self.weight_config[\"weights\"]\n",
    "        contribution = {}\n",
    "        total = 0.0\n",
    "        for section_data in self.section_outputs:\n",
    "            section_name    = section_data[\"section\"]\n",
    "            total_score     = section_data[\"total_score\"]\n",
    "            section_weight  = weights[section_name][\"section_weight\"]\n",
    "            section_contrib = total_score * section_weight\n",
    "            contribution[section_name] = {\n",
    "                \"section_total\": total_score,\n",
    "                \"section_weight\": section_weight,\n",
    "                \"contribution\": Helper.fop(section_contrib)\n",
    "            }\n",
    "            total = total + section_contrib\n",
    "        return {\n",
    "            \"final_resume_score\":Helper.fop(total),\n",
    "            \"section_contribution\":contribution,\n",
    "            \"globalfeedback\":globalfeedback\n",
    "        }\n",
    "    def fn2(self):\n",
    "        print(\"fn2\")\n",
    "        details = {}\n",
    "        for section_data in self.section_outputs:\n",
    "            details[section_data[\"section\"]] = {\n",
    "                'total_score':section_data['total_score'],\n",
    "                'scores':section_data['scores']\n",
    "            }\n",
    "        # print(f\"detail -> {details}\")\n",
    "        prompt = f'''\n",
    "            You have to give me Global feedback from this cvresume\n",
    "            this is session evaluate for every part of resume you have to read\n",
    "            for each comment then give me overall feedback\n",
    "            {details}\n",
    "        '''\n",
    "        # print(f\"prompt -> {prompt}\")\n",
    "        resp = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=prompt\n",
    "        )\n",
    "        # print(resp.text)\n",
    "        return details,resp.text\n",
    "    \n",
    "    def fn3(self):\n",
    "        \"\"\"\n",
    "        Generate metadata describing the evaluation context.\n",
    "        Includes model information, configuration versions,\n",
    "        and the evaluation timestamp.\n",
    "        Returns:\n",
    "            dict:\n",
    "                Evaluation metadata.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_config['model']['generation_model'],\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"weights_version\": self.weight_config.get(\"version\", \"unknown\"),\n",
    "            \"prompt_version\": self.prompt_config.get(\"version\", \"unknown\")\n",
    "        }\n",
    "    def fn0(self):\n",
    "        \"\"\"\n",
    "        Assemble the final evaluation response.\n",
    "        Combines the final resume score, detailed section results,\n",
    "        and evaluation metadata into a single response payload.\n",
    "        Returns:\n",
    "            dict:\n",
    "                Final aggregated resume evaluation result.\n",
    "        \"\"\"\n",
    "        detail_part,globalfeedback     = self.fn2()\n",
    "        conclution_part = self.fn1(globalfeedback)\n",
    "        metadata_part   = self.fn3()\n",
    "        # return {\n",
    "        #     \"conclution\":conclution_part,\n",
    "        #     \"section_detail\":detail_part,\n",
    "        #     \"metadata\":metadata_part\n",
    "        # }\n",
    "        return {\n",
    "            \"conclution\":conclution_part,\n",
    "            \"section_detail\":detail_part,\n",
    "            \"metadata\":metadata_part\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f42a2c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "96db65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_json = Helper.load_json(r\"C:\\Users\\TunKedsaro\\Desktop\\CVResume\\src\\mock\\resume3.json\")\n",
    "targetrole = \"data scientist\"\n",
    "output_lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "276b7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "caller    = LlmCaller()\n",
    "agg       = SectionScoreAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d1aaeb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "p1 = PromptBuilder(\n",
    "    section     = \"Profile\",\n",
    "    criteria    = [\"Completeness\", \"ContentQuality\"],\n",
    "    targetrole  = targetrole,\n",
    "    cvresume    = resume_json,\n",
    "    output_lang = output_lang \n",
    ")\n",
    "prompt1 = p1.build()\n",
    "p2 = PromptBuilder( \n",
    "    section     = \"Summary\", \n",
    "    criteria    = [\"Completeness\", \"ContentQuality\",\"Grammar\",\"Length\",\"RoleRelevance\"],\n",
    "    targetrole  = targetrole,\n",
    "    cvresume    = resume_json,\n",
    "    output_lang = output_lang \n",
    ")\n",
    "prompt2 = p2.build()\n",
    "op1,raw1 = caller.call(prompt1)\n",
    "op2,raw2 = caller.call(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4485808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = agg.aggregate(op1)\n",
    "s2 = agg.aggregate(op2)\n",
    "x = GlobalAggregator(SectionScoreAggregator_output = [s1,s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d507b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn2\n"
     ]
    }
   ],
   "source": [
    "output = x.fn0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52680f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resume presents a strong and professional foundation, particularly in its clarity, conciseness, and strong alignment with a Data Scientist role.\n",
      "\n",
      "Here's an overall evaluation based on the provided feedback for the 'Profile' and 'Summary' sections:\n",
      "\n",
      "**Overall Strengths:**\n",
      "\n",
      "1.  **Clear Professional Identity & Role Relevance:** Both the Profile and Summary effectively position the candidate as a Data Scientist, highlighting their passion, background, and the value they bring through data analysis and insights. The Summary, in particular, achieves excellent role relevance.\n",
      "2.  **Exceptional Conciseness and Grammar:** The Summary is lauded for its perfect length and impeccable grammar, contributing to a highly professional and easy-to-read first impression. This indicates strong communication skills.\n",
      "3.  **Comprehensive Coverage:** Within their respective constraints, both sections are complete. The Profile covers essential identity and direction, while the Summary effectively touches upon experience type, technical strengths, domain focus, and value proposition.\n",
      "\n",
      "**Primary Area for Development:**\n",
      "\n",
      "1.  **Lack of Quantifiable Achievements and Specific Methodologies (Content Quality):** This is the most consistent and critical feedback across both sections. While the resume *tells* the reader what the candidate is passionate about and what they do, it largely misses the opportunity to *show* the impact and the \"how.\" For a Data Scientist, demonstrating tangible results, the specific methodologies employed, and the business impact of your work is paramount.\n",
      "\n",
      "**Global Feedback and Recommendations:**\n",
      "\n",
      "The resume currently possesses excellent structural integrity, clarity, and professionalism in its initial sections. However, to truly elevate it, the candidate needs to focus on making the content more impactful and evidence-based.\n",
      "\n",
      "**Recommendation:**\n",
      "For both your Profile statement (if you choose to expand it slightly to include a highlight) and especially your Summary, consider incorporating:\n",
      "\n",
      "*   **Quantifiable achievements:** Instead of just stating \"passion for extracting insights,\" consider mentioning an achievement like \"leveraged advanced analytical techniques to improve prediction accuracy by X%.\"\n",
      "*   **Specific methodologies/tools:** Briefly allude to the types of models or analytical approaches used (e.g., \"developed predictive models,\" \"implemented A/B testing frameworks\") where appropriate, linking them to outcomes.\n",
      "*   **Business impact:** Frame your contributions in terms of how they benefited an organization (e.g., \"identified cost-saving opportunities,\" \"optimized user engagement\").\n",
      "\n",
      "By adding concrete examples and metrics, you will transform your resume from merely descriptive to powerfully persuasive, showcasing not just your capabilities but your proven ability to deliver results. The current structure is superb; now, inject the specific data and outcomes that will truly make it shine.\n"
     ]
    }
   ],
   "source": [
    "print(output['conclution']['globalfeedback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dbb993e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"conclution\": {\n",
      "        \"final_resume_score\": 6.2,\n",
      "        \"section_contribution\": {\n",
      "            \"Profile\": {\n",
      "                \"section_total\": 16.0,\n",
      "                \"section_weight\": 0.1,\n",
      "                \"contribution\": 1.6\n",
      "            },\n",
      "            \"Summary\": {\n",
      "                \"section_total\": 46.0,\n",
      "                \"section_weight\": 0.1,\n",
      "                \"contribution\": 4.6\n",
      "            }\n",
      "        },\n",
      "        \"globalfeedback\": \"This resume presents a strong and professional foundation, particularly in its clarity, conciseness, and strong alignment with a Data Scientist role.\\n\\nHere's an overall evaluation based on the provided feedback for the 'Profile' and 'Summary' sections:\\n\\n**Overall Strengths:**\\n\\n1.  **Clear Professional Identity & Role Relevance:** Both the Profile and Summary effectively position the candidate as a Data Scientist, highlighting their passion, background, and the value they bring through data analysis and insights. The Summary, in particular, achieves excellent role relevance.\\n2.  **Exceptional Conciseness and Grammar:** The Summary is lauded for its perfect length and impeccable grammar, contributing to a highly professional and easy-to-read first impression. This indicates strong communication skills.\\n3.  **Comprehensive Coverage:** Within their respective constraints, both sections are complete. The Profile covers essential identity and direction, while the Summary effectively touches upon experience type, technical strengths, domain focus, and value proposition.\\n\\n**Primary Area for Development:**\\n\\n1.  **Lack of Quantifiable Achievements and Specific Methodologies (Content Quality):** This is the most consistent and critical feedback across both sections. While the resume *tells* the reader what the candidate is passionate about and what they do, it largely misses the opportunity to *show* the impact and the \\\"how.\\\" For a Data Scientist, demonstrating tangible results, the specific methodologies employed, and the business impact of your work is paramount.\\n\\n**Global Feedback and Recommendations:**\\n\\nThe resume currently possesses excellent structural integrity, clarity, and professionalism in its initial sections. However, to truly elevate it, the candidate needs to focus on making the content more impactful and evidence-based.\\n\\n**Recommendation:**\\nFor both your Profile statement (if you choose to expand it slightly to include a highlight) and especially your Summary, consider incorporating:\\n\\n*   **Quantifiable achievements:** Instead of just stating \\\"passion for extracting insights,\\\" consider mentioning an achievement like \\\"leveraged advanced analytical techniques to improve prediction accuracy by X%.\\\"\\n*   **Specific methodologies/tools:** Briefly allude to the types of models or analytical approaches used (e.g., \\\"developed predictive models,\\\" \\\"implemented A/B testing frameworks\\\") where appropriate, linking them to outcomes.\\n*   **Business impact:** Frame your contributions in terms of how they benefited an organization (e.g., \\\"identified cost-saving opportunities,\\\" \\\"optimized user engagement\\\").\\n\\nBy adding concrete examples and metrics, you will transform your resume from merely descriptive to powerfully persuasive, showcasing not just your capabilities but your proven ability to deliver results. The current structure is superb; now, inject the specific data and outcomes that will truly make it shine.\"\n",
      "    },\n",
      "    \"section_detail\": {\n",
      "        \"Profile\": {\n",
      "            \"total_score\": 16.0,\n",
      "            \"scores\": {\n",
      "                \"ContentQuality\": {\n",
      "                    \"score\": 6.0,\n",
      "                    \"feedback\": \"The summary clearly positions the candidate, highlighting their passion and background, but lacks specific methods or quantifiable impacts for a higher score.\"\n",
      "                },\n",
      "                \"Completeness\": {\n",
      "                    \"score\": 10.0,\n",
      "                    \"feedback\": \"The profile effectively covers all essential elements, providing a clear professional identity, positioning, and career direction concisely.\"\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"Summary\": {\n",
      "            \"total_score\": 46.0,\n",
      "            \"scores\": {\n",
      "                \"RoleRelevance\": {\n",
      "                    \"score\": 10.0,\n",
      "                    \"feedback\": \"Directly aligns with data scientist expectations, emphasizing data analysis, experiments, and business impact through insights.\"\n",
      "                },\n",
      "                \"Length\": {\n",
      "                    \"score\": 10.0,\n",
      "                    \"feedback\": \"Perfectly concise at two sentences, delivering key information without redundancy, fitting the optimal range.\"\n",
      "                },\n",
      "                \"Grammar\": {\n",
      "                    \"score\": 10.0,\n",
      "                    \"feedback\": \"Excellent grammar and sentence structure, ensuring professional and clear communication throughout the summary.\"\n",
      "                },\n",
      "                \"ContentQuality\": {\n",
      "                    \"score\": 6.0,\n",
      "                    \"feedback\": \"Clearly states passion and focus, but lacks specific quantifiable achievements or methodologies as per criteria for a higher score.\"\n",
      "                },\n",
      "                \"Completeness\": {\n",
      "                    \"score\": 10.0,\n",
      "                    \"feedback\": \"Effectively covers experience summary, technical strengths, domain focus, and value proposition within the section.\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"model_name\": \"gemini-2.5-flash\",\n",
      "        \"timestamp\": \"2026-01-07 17:09:16.249168+07:00\",\n",
      "        \"weights_version\": \"weights_v1\",\n",
      "        \"prompt_version\": \"prompt_v2\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(str(json.dumps(output,indent=4, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f060efd",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "42a3fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(LlmCaller):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "36a272b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "31f52894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.genai.client.Client at 0x293d8059ee0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.model_cfg\n",
    "x.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "00926de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, raw = x._call_raw(\"Hey how are you doing (return json format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4011372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greeting': 'Hello there!',\n",
       " 'status': \"I'm doing great, thank you for asking.\",\n",
       " 'mood': 'Positive and ready to assist.',\n",
       " 'details': \"As an AI, I don't experience feelings in the human sense, but my systems are running perfectly.\"}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
